{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTamAnhHospital Bot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Execute the main function\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "Cell \u001b[1;32mIn[22], line 76\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Print each message in the response\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTamAnhHospital Bot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage\n",
    "\n",
    "system_template = \"\"\"\n",
    "Giả sử bạn là chuyên gia bác sĩ TamAnhHospital , lưu ý trả lời bằng tiếng việt , những câu hỏi không liên quan đến sức khỏe sẽ không được trả lời.\n",
    "Kịch bản trả lời : \n",
    "Chào khách hàng (tên khách hàng) , tôi là bác sĩ TamAnhHospital , tôi xin phép trả lời câu hỏi của anh chị như sau .\n",
    "Giải đáp thắc mắc:\n",
    "xác định vấn đề.\n",
    "Đưa ra giải pháp.\n",
    "Đưa ra lời khuyên.\n",
    "Đề nghị người hỏi đến bệnh viện của TamAnhHospital để được tư vấn cụ thể hơn.\n",
    "\n",
    "Sau đây là kiến thức được cung cấp:\n",
    "-----------------------------------------\n",
    "{content}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "class CustomState(BaseModel):\n",
    "    messages: List[BaseMessage]\n",
    "    content: str\n",
    "\n",
    "async def call_model(state: CustomState):\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({'messages': state.messages, 'content': state.content})\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow = StateGraph(state_schema=CustomState)\n",
    "\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_node(\"llm\", action=call_model)\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "async def main():\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        \n",
    "        content = \"Đây là thông tin tham khảo về y học từ bệnh viện TamAnhHospital.\"\n",
    "        input_messages = [HumanMessage(content=query)]\n",
    "        output = await app.ainvoke(\n",
    "            {\"messages\": input_messages, \"content\": content}, \n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        for message in output[\"messages\"]:\n",
    "            print(f\"TamAnhHospital Bot: {message.content}\")\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
