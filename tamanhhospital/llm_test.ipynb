{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CompiledStateGraph' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39mquery)]\n\u001b[0;32m     60\u001b[0m state \u001b[38;5;241m=\u001b[39m CustomState(messages\u001b[38;5;241m=\u001b[39minput_messages, content\u001b[38;5;241m=\u001b[39mcontent)\n\u001b[1;32m---> 61\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m response_messages \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;124m\"\u001b[39m, trim_messages(response_messages))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'CompiledStateGraph' object is not callable"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, trim_messages, BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import List\n",
    "\n",
    "system_template = \"\"\"\n",
    "Giả sử bạn là chuyên gia bác sĩ TamAnhHospital , lưu ý trả lời bằng tiếng việt , những câu hỏi không liên quan đến sức khỏe sẽ không được trả lời.\n",
    "Kịch bản trả lời : \n",
    "Chào khách hàng (tên khách hàng) , tôi là bác sĩ TamAnhHospital , tôi xin phép trả lời câu hỏi của anh chị như sau .\n",
    "Giải đáp thắc mắc:\n",
    "xác định vấn đề.\n",
    "Đưa ra giải pháp.\n",
    "Đưa ra lời khuyên.\n",
    "Đề nghị người hỏi đến bệnh viện của TamAnhHospital để được tư vấn cụ thể hơn.\n",
    "\n",
    "Sau đây là kiến thức được cung cấp:\n",
    "-----------------------------------------\n",
    "{content}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_template,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "class CustomState(BaseModel):\n",
    "    messages: List[BaseMessage]\n",
    "    content: str\n",
    "    \n",
    "async def call_model(state: CustomState):\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"messages\": state.messages, \"content\": state.content})\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_node(\"llm\", action=call_model)\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Tôi bị đau bụng\"\n",
    "content = \"Đây là thông tin tham khảo về y học từ bệnh viện TamAnhHospital.\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "for chunk,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
